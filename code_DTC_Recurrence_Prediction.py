# -*- coding: utf-8 -*-
"""CS471_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w6ZBWbROGveXV07lFhTASBN3XMqms5A-
"""

from google.colab import drive
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn import tree
from sklearn.metrics import accuracy_score, recall_score, classification_report
import seaborn as sns
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.pipeline import Pipeline
from sklearn.datasets import make_classification
from sklearn.metrics import confusion_matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from imblearn.over_sampling import SMOTE
from sklearn.metrics import make_scorer, recall_score
from sklearn.preprocessing import OrdinalEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from scipy.stats import chi2_contingency
from sklearn.feature_selection import mutual_info_classif
from scipy.stats import pointbiserialr

drive.mount('/content/gdrive')

data = pd.read_csv("gdrive/MyDrive/CS471_Final_Project/Thyroid_Diff.csv")
#data = pd.read_csv("gdrive/MyDrive/Thyroid_Diff.csv") # to use csv in Sarahs google drive

data.head()

#EDA step:

#Dataframe information
data.info()

#count data values per category for Recurred column
data.value_counts("Recurred")

#identify minimum and maximum age of patients in the dataset
min_value = data['Age'].min()

max_value = data['Age'].max()
print(min_value)
print(max_value)

#Create histogram to show age trend for the patients in the data set
data.Age.hist(bins=30)
plt.xlabel('Number of patients based on Age')
plt.ylabel('count');

#count data values per category for Gender column
data.value_counts("Gender")
#significantly greater number of female than male patients

#count data values per category for Smoking column
data.value_counts("Smoking")

#count data values per category for Hx smoking column
data.value_counts("Hx Smoking")

#count data values per category for Hx Radiotherapy column
data.value_counts("Hx Radiothreapy")

#count data values per category for Thyroid Function column
data.value_counts("Thyroid Function")

#count data values per category for Physical Examination column
data.value_counts("Physical Examination")

#count data values per category for Adenopathy column
data.value_counts("Adenopathy")

#count data values per category for Pathology column
data.value_counts("Pathology")

#count data values per category for Focality column
data.value_counts("Focality")

#count data values per category for Risk column
data.value_counts("Risk")

#count data values per category for T column
data.value_counts("T")

#count data values per category for N column
data.value_counts("N")

#count data values per category for M column
data.value_counts("M")

#count data values per category for Stage column
data.value_counts("Stage")

#count data values per category for Response column
data.value_counts("Response")

# Initialize an empty list to store p-values
p_values = []

features = [
    "Smoking", "Hx Smoking", "Hx Radiothreapy", "Adenopathy",
    "Thyroid Function", "Pathology", "Focality", "Risk",
    "T", "N", "M", "Stage", "Response",
    "Physical Examination", "Gender"
]
target = "Recurred"

# Perform Chi-Square Test for each feature
for feature in features:
    # Create a contingency table
    contingency_table = pd.crosstab(data[feature], data[target])

    # Perform the Chi-Square test
    chi2, p_value, dof, expected = chi2_contingency(contingency_table)

    # Append the p-value to the list
    p_values.append(p_value)

# Convert p-values to a DataFrame for better visualization
p_values_df = pd.DataFrame(p_values, index=features, columns=[target])

# Plot the heatmap using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(p_values_df, annot=True, cmap='coolwarm', vmin=0, vmax=1, cbar_kws={'label': 'p-value'})
plt.title(f"Chi-Square p-values between Categorical Features and {target}")
plt.show()

#Pre-processing step:

# Spliting the data into Training, Validation, and Testing:
X = data.drop(columns='Recurred')
y = data['Recurred']

# Define columns for ordinal encoding (columns with a natural order)
ordinal_columns = [
    "Risk", "Stage", "T", "N", "M", "Response"
]

# Define columns for label encoding (nominal data without an inherent order)
nominal_columns = [
    "Smoking", "Hx Smoking", "Hx Radiothreapy", "Adenopathy",
    "Thyroid Function", "Pathology", "Focality",
    "Physical Examination", "Gender"
]

# Initialize OrdinalEncoder for ordered columns and LabelEncoder for nominal columns
ordinal_encoder = OrdinalEncoder()
label_encoder = LabelEncoder()

# Apply OrdinalEncoder to the ordinal columns
X[ordinal_columns] = ordinal_encoder.fit_transform(X[ordinal_columns])

# Apply LabelEncoder to the nominal columns one by one
for column in nominal_columns:
    X[column] = label_encoder.fit_transform(X[column])

#encode the target variable
y_encoded = label_encoder.fit_transform(y)


X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y, random_state=42)

#Algorithm 1 (rule based approach):
#Given the dataset as input, returns predictions about recurrence
def ruleBasedAlgorithm(data):
  Y_Pred = []

  for index, row in data.iterrows():
    # Check Response and Risk features for poor response to treatment or high risk of recurrence
    if row['Response'] == 1 or row['Response'] == 2 or row['Response'] == 3 and row['Risk'] == 1 or row['Risk'] == 2:
      Y_Pred.append(1)
    # Check N and M features next to see if the tumor spread to other parts of the body
    elif row['N'] == 1 or row['N'] == 2 or row['M'] == 1:
      Y_Pred.append(1)
    # Check Stage feature next to check cancer stage
    elif row['Stage'] == 2 or row['Stage'] == 3 or row['Stage'] == 4:
       Y_Pred.append(1)
    # Check T feature next to check tumor size
    elif (row['T'] == 2 or row['T'] == 3 or row['T'] == 4 or row['T'] == 5 or row['T'] == 6):
      Y_Pred.append(1)
    else:
      Y_Pred.append(0)


  return Y_Pred

# Calculate the recall of the predictions by comparing them to the true labels
Y_Pred = ruleBasedAlgorithm(X_train)
recall = recall_score(y_train, Y_Pred)
print(f"Training recall: {recall:.4f}")

# Performance on the testing set
Y_Pred = ruleBasedAlgorithm(X_test)
recall = recall_score(y_test, Y_Pred)
print(f"Test recall: {recall:.4f}")

print("Classification Report for Testing Set using Rule Based Model:\n", classification_report(y_test, Y_Pred))

print("Confusion Matrix for Testing Set using Rule based Model")
cm = confusion_matrix(y_test, Y_Pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()

# Second version of rule based algorithm using smoking since that was the most important feature of the decision tree
def ruleBasedAlgorithm2(data):
  Y_Pred = []

  for index, row in data.iterrows():
    # Check smoking feature
    if row['Smoking'] == 1:
      Y_Pred.append(1)
    # Check Response and Risk features for poor response to treatment or high risk of recurrence
    elif row['Response'] == 1 or row['Response'] == 2 or row['Response'] == 3 and row['Risk'] == 1 or row['Risk'] == 2:
      Y_Pred.append(1)
    # Check Stage feature next to check cancer stage
    elif row['Stage'] == 2 or row['Stage'] == 3 or row['Stage'] == 4:
       Y_Pred.append(1)
    else:
      Y_Pred.append(0)


  return Y_Pred

# Calculate the recall of the predictions by comparing them to the true labels
Y_Pred = ruleBasedAlgorithm2(X_train)
recall = recall_score(y_train, Y_Pred)
print(f"Second Rule Based Model Training Recall: {recall:.4f}")

# Performance on the testing set
Y_Pred = ruleBasedAlgorithm2(X_test)
recall = recall_score(y_test, Y_Pred)
print(f"Second Rule Based Model Test Recall: {recall:.4f}")

print("Second Rule Based Model Classification Report for Testing:\n", classification_report(y_test, Y_Pred))

# Confusion matrix for test set
print("Second Rule Based Model Confusion Matrix for Test Set")
cm = confusion_matrix(y_test, Y_Pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()

# Random forest model without tuning

# Random Forest results before tuning n_estimators
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print("Recall on Testing Set for Untuned RandomForest Model (unbalanced data set):", recall_score(y_test, y_pred))
print("Classification Report on Testing Set for Untuned RandomForest Model (unbalanced data set):\n", classification_report(y_test, y_pred))

# Tuning the hyperparameters:

recall_scorer = make_scorer(recall_score, pos_label=1)

pipeline = Pipeline([
    ('feature_selection', SelectKBest(score_func=f_classif)),  # Feature selection step
    ('classifier', RandomForestClassifier(random_state=42, bootstrap=True))    # RandomForest classifier
])

param_grid = {
    'classifier__n_estimators': range(5, 50), # number of trees in random forest
    'classifier__max_depth': range(2,5),  # Tune max_depth from 1 to 15
    'feature_selection__k': range(5, 10)  # Tune max_features from 5 to 10
}

grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),            # 5-fold cross-validation
    scoring=recall_scorer,  # Use recall as the metric
    n_jobs=-1        # Use all available cores
)

grid_search.fit(X_train, y_train)

# Extract the results
results = grid_search.cv_results_

# Get the best parameters and best recall score
best_recall = grid_search.best_score_
best_params = grid_search.best_params_

# Print the results
print(f"Best Recall in CV for RandomForest Model (Unbalanced data set): {best_recall:.4f}")
print(f"Best Combination in CV for RandomForest Model (Unbalanced data set): {best_params}")

# Create RandomForest with best n_estimators

best_model = grid_search.best_estimator_

y_pred = best_model.predict(X_test)

print("Recall on Testing Set for Tuned RandomForest Model (unbalanced data set):", recall_score(y_test, y_pred))
print("Classification Report on Testing Set for Tuned RandomForest Model (unbalanced data set):\n", classification_report(y_test, y_pred))
print("Confusion Matrix for Test Set using Tuned RandomForest Model (unbalanced data set)")
cm = confusion_matrix(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot()
plt.show()

# Extract the classifier from the pipeline
classifier = best_model.named_steps['classifier']

# Check if the classifier has the 'feature_importances_' attribute (e.g., RandomForest, DecisionTree)
if hasattr(classifier, 'feature_importances_'):
    # Get the feature importances
    feature_importances = classifier.feature_importances_

    # Sort the features based on their importance
    indices = np.argsort(feature_importances)[::-1]

    # Print the top k features (e.g., top 10)
    top_k = best_params['feature_selection__k']
    top_features = [(X.columns[i], feature_importances[i]) for i in indices[:top_k]]

    # Print the top k features
    print(f"Top {top_k} Features by Importance for Tuned RandomForest Model (Unbalanced data):")
    for feature, importance in top_features:
        print(f"{feature}: {importance:.4f}")
else:
    print("The classifier does not have 'feature_importances_' attribute.")

# Plot the top k features on a bar graph
plt.figure(figsize=(10, 6))
plt.barh([feature for feature, _ in top_features], [importance for _, importance in top_features], align='center')
plt.xlabel('Feature Importance')
plt.title('Top Features by Importance for Tuned RandomForest Model (Unbalanced data)')
plt.gca().invert_yaxis()  # To display the most important feature on top
plt.show()

#Try balancing out the data using SMOTE and see results for Random Forest Model
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Random Forest results before tuning n_estimators
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train_resampled, y_train_resampled)

y_pred = clf.predict(X_test)

print("Recall on Testing Set for Untuned RandomForest Model (balanced data set):", recall_score(y_test, y_pred))
print("Classification Report on Testing Set for Untuned RandomForest Model (balanced data set):\n", classification_report(y_test, y_pred))

# Tuning the hyperparameters:

recall_scorer = make_scorer(recall_score, pos_label=1)

pipeline = Pipeline([
    ('feature_selection', SelectKBest(score_func=f_classif)),  # Feature selection step
    ('classifier', RandomForestClassifier(random_state=42, bootstrap=True))    # RandomForest classifier
])

param_grid = {
    'classifier__n_estimators': range(5, 50), # number of trees in random forest
    'classifier__max_depth': range(2,5),  # Tune max_depth from 1 to 15
    'feature_selection__k': range(5, 10)  # Tune max_features from 5 to 10
}

grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),            # 5-fold cross-validation
    scoring=recall_scorer,  # Use recall as the metric
    n_jobs=-1        # Use all available cores
)

grid_search.fit(X_train_resampled, y_train_resampled)

# Extract the results
results = grid_search.cv_results_

# Get the best parameters and best recall score
best_recall = grid_search.best_score_
best_params = grid_search.best_params_

# Print the results
print(f"Best Recall in CV for RandomForest Model (Balanced data set): {best_recall:.4f}")
print(f"Best Combination in CV for RandomForest Model (Balanced data set): {best_params}")

# Create RandomForest with best n_estimators

best_model = grid_search.best_estimator_

y_pred = best_model.predict(X_test)

print("Recall on Testing Set for Tuned RandomForest Model (Balanced data set):", recall_score(y_test, y_pred))
print("Classification Report on Testing Set for Tuned RandomForest Model (Balanced data set):\n", classification_report(y_test, y_pred))
print("Confusion Matrix for Test Set using Tuned RandomForest Model (Balanced data set)")
cm = confusion_matrix(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot()
plt.show()

# Extract the classifier from the pipeline
classifier = best_model.named_steps['classifier']

# Check if the classifier has the 'feature_importances_' attribute (e.g., RandomForest, DecisionTree)
if hasattr(classifier, 'feature_importances_'):
    # Get the feature importances
    feature_importances = classifier.feature_importances_

    # Sort the features based on their importance
    indices = np.argsort(feature_importances)[::-1]

    # Print the top k features (e.g., top 10)
    top_k = best_params['feature_selection__k']
    top_features = [(X.columns[i], feature_importances[i]) for i in indices[:top_k]]

    # Print the top k features
    print(f"Top {top_k} Features by Importance for Tuned RandomForest Model (Balanced data):")
    for feature, importance in top_features:
        print(f"{feature}: {importance:.4f}")
else:
    print("The classifier does not have 'feature_importances_' attribute.")

# Plot the top k features on a bar graph
plt.figure(figsize=(10, 6))
plt.barh([feature for feature, _ in top_features], [importance for _, importance in top_features], align='center')
plt.xlabel('Feature Importance')
plt.title('Top Features by Importance for Tuned RandomForest Model (Balanced data)')
plt.gca().invert_yaxis()  # To display the most important feature on top
plt.show()

# Algorithm 3 Decision Tree:

#untuned decision tree model
untuned_model = DecisionTreeClassifier(random_state=42)

untuned_model.fit(X_train, y_train)

y_pred = untuned_model.predict(X_test)

print("Recall on Testing Set for Untuned Decision Tree Model (Unbalanced data set):", recall_score(y_test, y_pred))
print("Classification Report on Testing Set for Untuned Decision Tree Model (Unbalanced data set):\n", classification_report(y_test, y_pred))

recall_scorer = make_scorer(recall_score, pos_label=1)

pipeline = Pipeline([
    ('feature_selection', SelectKBest(score_func=f_classif)),  # Feature selection step
    ('classifier', DecisionTreeClassifier(random_state=42))    # Decision Tree classifier
])

param_grid = {
    'classifier__max_depth': range(2,10),  # Tune max_depth from 1 to 15
    'feature_selection__k': range(3, 9)  # Tune max_features from 5 to 10
}

grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),            # 3-fold cross-validation
    scoring=recall_scorer,  # Use recall as the metric
    n_jobs=-1        # Use all available cores
)

grid_search.fit(X_train, y_train)

results = grid_search.cv_results_

# Get the best parameters and best recall score
best_recall = grid_search.best_score_
best_params = grid_search.best_params_

# Print the results
print(f"Best Recall in CV for Decision Tree Model (Unbalanced data set): {best_recall:.4f}")
print(f"Best Combination in CV for Decision Tree Model (Unbalanced data set): {best_params}")

best_model = grid_search.best_estimator_

y_pred = best_model.predict(X_test)

print("Recall on Testing Set for Tuned Decision Tree Model (Unbalanced data set):", recall_score(y_test, y_pred))
print("Classification Report on Training Set for Tuned Decision Tree Model (Unbalanced data set):\n", classification_report(y_test, y_pred))

# Extract the classifier from the pipeline
classifier = best_model.named_steps['classifier']

# Check if the classifier has the 'feature_importances_' attribute (e.g., RandomForest, DecisionTree)
if hasattr(classifier, 'feature_importances_'):
    # Get the feature importances
    feature_importances = classifier.feature_importances_

    # Sort the features based on their importance
    indices = np.argsort(feature_importances)[::-1]

    # Print the top k features (e.g., top 10)
    top_k = best_params['feature_selection__k']
    top_features = [(X.columns[i], feature_importances[i]) for i in indices[:top_k]]

    # Print the top k features
    print(f"Top {top_k} Features by Importance for Tuned Deicsion Tree Model (Unbalanced data):")
    for feature, importance in top_features:
        print(f"{feature}: {importance:.4f}")
else:
    print("The classifier does not have 'feature_importances_' attribute.")

# Plot the top k features on a bar graph
plt.figure(figsize=(10, 6))
plt.barh([feature for feature, _ in top_features], [importance for _, importance in top_features], align='center')
plt.xlabel('Feature Importance')
plt.title('Top Features by Importance for Tuned Deicsion Tree Model (Unbalanced data)')
plt.gca().invert_yaxis()  # To display the most important feature on top
plt.show()

class_names = [str(cls) for cls in [1, 0]]  # Convert class names to strings


# Assuming `pipeline` is your pipeline object
decision_tree_model = best_model.named_steps['classifier']

# Now plot the tree
plt.figure(figsize=(50,50))  # Adjust the size as needed
plot_tree(decision_tree_model,
          feature_names=X.columns,  # If your features are in a DataFrame
          class_names=class_names,     # Class names in your target variable
          filled=True,                    # Color nodes to indicate purity
          rounded=True,                   # Rounded corners for clarity
          fontsize=12)
plt.show()

#Try balancing out the data using SMOTE and see results for Decision Tree Model
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

#untuned decision tree model
untuned_model = DecisionTreeClassifier(random_state=42)

untuned_model.fit(X_train_resampled, y_train_resampled)

y_pred = untuned_model.predict(X_test)

print("Recall on Testing Set for Untuned Decision Tree Model (Balanced data set):", recall_score(y_test, y_pred))
print("Classification Report on Testing Set for Untuned Decision Tree Model (Balanced data set):\n", classification_report(y_test, y_pred))

#tuning decision tree model for balanced data set

recall_scorer = make_scorer(recall_score, pos_label=1)

pipeline = Pipeline([
    ('feature_selection', SelectKBest(score_func=f_classif)),  # Feature selection step
    ('classifier', DecisionTreeClassifier(random_state=42))    # Decision Tree classifier
])

param_grid = {
    'classifier__max_depth': range(2,10),  # Tune max_depth from 1 to 15
    'feature_selection__k': range(3, 9)  # Tune max_features from 5 to 10
}

grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),            # 3-fold cross-validation
    scoring=recall_scorer,  # Use recall as the metric
    n_jobs=-1        # Use all available cores
)

grid_search.fit(X_train_resampled, y_train_resampled)

results = grid_search.cv_results_

# Get the best parameters and best recall score
best_recall = grid_search.best_score_
best_params = grid_search.best_params_

# Print the results
print(f"Best Recall in CV for Decision Tree Model (Balanced data set): {best_recall:.4f}")
print(f"Best Combination in CV for Decision Tree Model (Balanced data set): {best_params}")

best_model = grid_search.best_estimator_

y_pred = best_model.predict(X_test)

print("Recall on Testing Set for Tuned Decision Tree Model (Balanced data set):", recall_score(y_test, y_pred))
print("Classification Report on Training Set for Tuned Decision Tree Model (Balanced data set):\n", classification_report(y_test, y_pred))

# Extract the classifier from the pipeline
classifier = best_model.named_steps['classifier']

# Check if the classifier has the 'feature_importances_' attribute (e.g., RandomForest, DecisionTree)
if hasattr(classifier, 'feature_importances_'):
    # Get the feature importances
    feature_importances = classifier.feature_importances_

    # Sort the features based on their importance
    indices = np.argsort(feature_importances)[::-1]

    # Print the top k features (e.g., top 10)
    top_k = best_params['feature_selection__k']
    top_features = [(X.columns[i], feature_importances[i]) for i in indices[:top_k]]

    # Print the top k features
    print(f"Top {top_k} Features by Importance for Tuned Deicsion Tree Model (Unbalanced data):")
    for feature, importance in top_features:
        print(f"{feature}: {importance:.4f}")
else:
    print("The classifier does not have 'feature_importances_' attribute.")

# Plot the top k features on a bar graph
plt.figure(figsize=(10, 6))
plt.barh([feature for feature, _ in top_features], [importance for _, importance in top_features], align='center')
plt.xlabel('Feature Importance')
plt.title('Top Features by Importance for Tuned Deicsion Tree Model (Unbalanced data)')
plt.gca().invert_yaxis()  # To display the most important feature on top
plt.show()

class_names = [str(cls) for cls in [1, 0]]  # Convert class names to strings


# Assuming `pipeline` is your pipeline object
decision_tree_model = best_model.named_steps['classifier']

# Now plot the tree
plt.figure(figsize=(50,50))  # Adjust the size as needed
plot_tree(decision_tree_model,
          feature_names=X.columns,  # If your features are in a DataFrame
          class_names=class_names,     # Class names in your target variable
          filled=True,                    # Color nodes to indicate purity
          rounded=True,                   # Rounded corners for clarity
          fontsize=12)
plt.show()